{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bc5986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "from nemo.collections.tts.models.base import SpectrogramGenerator, Vocoder, TextToWaveform\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from nemo.collections.tts.modules.hifigan_modules import Generator as Hifigan_generator\n",
    "\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "import librosa\n",
    "import os\n",
    "from nemo.collections.tts.models import TwoStagesModel\n",
    "from nemo.collections.asr.modules import AudioToMelSpectrogramPreprocessor\n",
    "from nemo.collections.asr.parts.preprocessing.features import WaveformFeaturizer\n",
    "from nemo.collections.asr.models import EncDecSpeakerLabelModel\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ead92ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(spec_gen_model, vocoder_model, str_input, speaker = None):\n",
    "    parser_model = spec_gen_model\n",
    "    with torch.no_grad():\n",
    "        parsed = parser_model.parse(str_input)\n",
    "        if speaker is not None:\n",
    "            speaker = torch.tensor([speaker]).long().cuda()\n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed, speaker = speaker)\n",
    "        if isinstance(vocoder_model, Hifigan_generator):\n",
    "            audio = vocoder_model(x=spectrogram.half()).squeeze(1)\n",
    "        else:\n",
    "            audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "        \n",
    "    if spectrogram is not None:\n",
    "        if isinstance(spectrogram, torch.Tensor):\n",
    "            spectrogram = spectrogram.to('cpu').numpy()\n",
    "        if len(spectrogram.shape) == 3:\n",
    "            spectrogram = spectrogram[0]\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return spectrogram, audio\n",
    "\n",
    "\n",
    "wav_featurizer = WaveformFeaturizer(sample_rate=44100, int_values=False, augmentor=None)\n",
    "mel_processor = AudioToMelSpectrogramPreprocessor(\n",
    "        window_size = None,\n",
    "        window_stride = None,\n",
    "        sample_rate=44100,\n",
    "        n_window_size=2048,\n",
    "        n_window_stride=512,\n",
    "        window=\"hann\",\n",
    "        normalize=None,\n",
    "        n_fft=None,\n",
    "        preemph=None,\n",
    "        features=80,\n",
    "        lowfreq=0,\n",
    "        highfreq=None,\n",
    "        log=True,\n",
    "        log_zero_guard_type=\"add\",\n",
    "        log_zero_guard_value=1e-05,\n",
    "        dither=0.0,\n",
    "        pad_to=1,\n",
    "        frame_splicing=1,\n",
    "        exact_pad=False,\n",
    "        stft_exact_pad=False,\n",
    "        stft_conv=False,\n",
    "        pad_value=0,\n",
    "        mag_power=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {'linvocoder':  {'_target_': 'nemo.collections.tts.models.two_stages.GriffinLimModel',\n",
    "                     'cfg': {'n_iters': 64, 'n_fft': 2048, 'l_hop': 512}},\n",
    "       'mel2spec': {'_target_': 'nemo.collections.tts.models.two_stages.MelPsuedoInverseModel',\n",
    "                   'cfg': {'sampling_rate': 44100, 'n_fft': 2048, \n",
    "                           'mel_fmin': 0, 'mel_fmax': None, 'mel_freq': 80}}}\n",
    "vocoder_gl = TwoStagesModel(cfg).eval().cuda()\n",
    "\n",
    "# mixed vocoder - trained on multiple speakers\n",
    "vocoder = HifiGanModel.load_from_checkpoint(\"/home/pneekhara/PreTrainedModels/HifiGan--val_loss=0.09-epoch=649-last.ckpt\")\n",
    "vocoder.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10abec0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spec_model_original = FastPitchModel.restore_from('/home/pneekhara/PreTrainedModels/FastPitch.nemo')\n",
    "spec_model_original.eval().cuda()\n",
    "\n",
    "data_dir = \"/home/pneekhara/Datasets/78419/Hi_Fi_TTS_v_0_backup\"\n",
    "experiment_base_dir = \"/home/pneekhara/ExperimentsAutomatedNewPitchStats/\"\n",
    "clean_other_mapping = {\n",
    "    92 : 'clean',\n",
    "    6097 : 'clean'\n",
    "}\n",
    "\n",
    "full_data_ckpts = {\n",
    "    92 : '/home/pneekhara/Checkpoints/FastPitchSpeaker92Epoch999.ckpt',\n",
    "    6097 : '/home/pneekhara/Checkpoints/FastPitch6097Epoch999.ckpt'\n",
    "}\n",
    "\n",
    "finetuning_ckpts = {\n",
    "    \"200\" : \"/home/pneekhara/CheckpointsOverfitting/FastPitch--v_loss=1.19-epoch=199-last.ckpt\",\n",
    "    \"1000\" : \"/home/pneekhara/CheckpointsOverfitting/FastPitch--v_loss=1.16-epoch=999-last.ckpt\",\n",
    "    \"10,000\": \"/home/pneekhara/CheckpointsOverfitting/FastPitch--v_loss=1.48-epoch=9999-last.ckpt\"\n",
    "}\n",
    "\n",
    "num_val = 7\n",
    "\n",
    "for speaker in [92]:\n",
    "    manifest_path = os.path.join(data_dir, \"{}_manifest_{}_{}.json\".format(speaker, clean_other_mapping[speaker], \"dev\"))\n",
    "    val_records = []\n",
    "    with open(manifest_path, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            val_records.append( json.loads(line) )\n",
    "            if len(val_records) >= num_val:\n",
    "                break\n",
    "    \n",
    "    for vidx, val_record in enumerate(val_records):\n",
    "        print(\"Audio path:\", val_record['audio_filepath'] )\n",
    "        audio_path = os.path.join( data_dir, val_record['audio_filepath'] )\n",
    "        \n",
    "        real_wav = wav_featurizer.process(audio_path)\n",
    "        real_mel, _ = mel_processor.get_features(real_wav[None], torch.tensor([[real_wav.shape[0]]]).long() )\n",
    "        real_mel = real_mel[0].cuda()\n",
    "        with torch.no_grad():\n",
    "            vocoded_audio_real = vocoder.convert_spectrogram_to_audio(spec=real_mel).cpu().numpy()\n",
    "        print (vidx+1, val_record['text'])\n",
    "        print(vidx+1, \"Ground Truth Audio for speaker:\", speaker)\n",
    "        ipd.display(ipd.Audio(real_wav, rate=44100))\n",
    "        print(vidx+1, \"Ground Truth spectrogram vocoded (HiFiGAN):\", speaker)\n",
    "        ipd.display(ipd.Audio(vocoded_audio_real, rate=44100))\n",
    "        \n",
    "        print (\"----------------------\")\n",
    "    \n",
    "        for key in finetuning_ckpts:      \n",
    "            spec_model = FastPitchModel.load_from_checkpoint(finetuning_ckpts[key]).eval().cuda()\n",
    "            _speaker=None\n",
    "            \n",
    "            print (vidx+1, \"Synthesized. Finetuned for {} epochs | \".format(key) , val_record['text'])\n",
    "            _, audio = infer(spec_model, vocoder, val_record['text'], speaker = _speaker)\n",
    "            ipd.display(ipd.Audio(audio, rate=44100))\n",
    "        print (\"*******************\"*5)\n",
    "        print (\"*******************\"*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff9466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemoenv",
   "language": "python",
   "name": "nemoenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
