{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bc5986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "from nemo.collections.tts.models.base import SpectrogramGenerator, Vocoder, TextToWaveform\n",
    "from nemo.collections.tts.models import FastPitchModel, Tacotron2Model\n",
    "from nemo.collections.tts.modules.hifigan_modules import Generator as Hifigan_generator\n",
    "\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "import librosa\n",
    "import os\n",
    "from nemo.collections.tts.models import TwoStagesModel\n",
    "from nemo.collections.asr.modules import AudioToMelSpectrogramPreprocessor\n",
    "from nemo.collections.asr.parts.preprocessing.features import WaveformFeaturizer\n",
    "from nemo.collections.asr.models import EncDecSpeakerLabelModel\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ead92ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_taco(spec_gen_model, vocoder_model, str_input, speaker = None):\n",
    "    parser_model = spec_gen_model\n",
    "    with torch.no_grad():\n",
    "        parsed = parser_model.parse(str_input)\n",
    "        if speaker is not None:\n",
    "            speaker = torch.tensor([speaker]).long().cuda()\n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed)\n",
    "        if isinstance(vocoder_model, Hifigan_generator):\n",
    "            audio = vocoder_model(x=spectrogram.half()).squeeze(1)\n",
    "        else:\n",
    "            audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "        \n",
    "    if spectrogram is not None:\n",
    "        if isinstance(spectrogram, torch.Tensor):\n",
    "            spectrogram = spectrogram.to('cpu').numpy()\n",
    "        if len(spectrogram.shape) == 3:\n",
    "            spectrogram = spectrogram[0]\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return spectrogram, audio\n",
    "\n",
    "\n",
    "def infer(spec_gen_model, vocoder_model, str_input, speaker = None):\n",
    "    parser_model = spec_gen_model\n",
    "    with torch.no_grad():\n",
    "        parsed = parser_model.parse(str_input)\n",
    "        if speaker is not None:\n",
    "            speaker = torch.tensor([speaker]).long().cuda()\n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed, speaker = speaker)\n",
    "        if isinstance(vocoder_model, Hifigan_generator):\n",
    "            audio = vocoder_model(x=spectrogram.half()).squeeze(1)\n",
    "        else:\n",
    "            audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "        \n",
    "    if spectrogram is not None:\n",
    "        if isinstance(spectrogram, torch.Tensor):\n",
    "            spectrogram = spectrogram.to('cpu').numpy()\n",
    "        if len(spectrogram.shape) == 3:\n",
    "            spectrogram = spectrogram[0]\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return spectrogram, audio\n",
    "\n",
    "def get_best_ckpt(experiment_base_dir, new_speaker_id, duration_mins, mixing_enabled, original_speaker_id):\n",
    "    if not mixing_enabled:\n",
    "        exp_dir = \"{}/{}_to_{}_no_mixing_{}_mins\".format(experiment_base_dir, original_speaker_id, new_speaker_id, duration_mins)\n",
    "    else:\n",
    "        exp_dir = \"{}/{}_to_{}_mixing_{}_mins\".format(experiment_base_dir, original_speaker_id, new_speaker_id, duration_mins)\n",
    "    \n",
    "    return _get_best_ckpt_from_experiment(exp_dir)\n",
    "    \n",
    "\n",
    "def _get_best_ckpt_from_experiment(exp_dir):\n",
    "    ckpt_candidates = []\n",
    "    last_ckpt = None\n",
    "    for root, dirs, files in os.walk(exp_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".ckpt\"):\n",
    "                try:\n",
    "                    val_error = float(file.split(\"v_loss=\")[1].split(\"-epoch\")[0])\n",
    "                except:\n",
    "                    val_error = 0.0\n",
    "                if \"last\" in file:\n",
    "                    last_ckpt = os.path.join(root, file)\n",
    "                ckpt_candidates.append( (val_error, os.path.join(root, file)))\n",
    "    ckpt_candidates.sort()\n",
    "    \n",
    "    return ckpt_candidates, last_ckpt\n",
    "\n",
    "wav_featurizer = WaveformFeaturizer(sample_rate=44100, int_values=False, augmentor=None)\n",
    "mel_processor = AudioToMelSpectrogramPreprocessor(\n",
    "        window_size = None,\n",
    "        window_stride = None,\n",
    "        sample_rate=44100,\n",
    "        n_window_size=2048,\n",
    "        n_window_stride=512,\n",
    "        window=\"hann\",\n",
    "        normalize=None,\n",
    "        n_fft=None,\n",
    "        preemph=None,\n",
    "        features=80,\n",
    "        lowfreq=0,\n",
    "        highfreq=None,\n",
    "        log=True,\n",
    "        log_zero_guard_type=\"add\",\n",
    "        log_zero_guard_value=1e-05,\n",
    "        dither=0.0,\n",
    "        pad_to=1,\n",
    "        frame_splicing=1,\n",
    "        exact_pad=False,\n",
    "        stft_exact_pad=False,\n",
    "        stft_conv=False,\n",
    "        pad_value=0,\n",
    "        mag_power=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocoder = Hifigan_generator(\n",
    "#     resblock=1,\n",
    "#     upsample_rates=[8, 8, 4, 2],\n",
    "#     upsample_kernel_sizes=[16, 16, 4, 4],\n",
    "#     upsample_initial_channel=512,\n",
    "#     resblock_kernel_sizes=[3, 7, 11],\n",
    "#     resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    "# )\n",
    "# nemo_gen_keys = [k for k in vocoder.state_dict().keys()]\n",
    "# adlr_gen_ckpt = torch.load(\"/home/pneekhara/Checkpoints/g_01224000\")['generator']\n",
    "# adlr_gen_keys = adlr_gen_ckpt.keys()\n",
    "\n",
    "# new_nemo_ckpt = {nemo_key: adlr_gen_ckpt[adlr_key] for adlr_key, nemo_key in zip(adlr_gen_keys, nemo_gen_keys)}\n",
    "# vocoder.load_state_dict(new_nemo_ckpt)\n",
    "# vocoder = vocoder.cuda().eval().half()\n",
    "\n",
    "cfg = {'linvocoder':  {'_target_': 'nemo.collections.tts.models.two_stages.GriffinLimModel',\n",
    "                     'cfg': {'n_iters': 64, 'n_fft': 2048, 'l_hop': 512}},\n",
    "       'mel2spec': {'_target_': 'nemo.collections.tts.models.two_stages.MelPsuedoInverseModel',\n",
    "                   'cfg': {'sampling_rate': 44100, 'n_fft': 2048, \n",
    "                           'mel_fmin': 0, 'mel_fmax': None, 'mel_freq': 80}}}\n",
    "vocoder_gl = TwoStagesModel(cfg).eval().cuda()\n",
    "\n",
    "vocoder = HifiGanModel.load_from_checkpoint(\"/home/pneekhara/PreTrainedModels/HifiGan--val_loss=0.08-epoch=899.ckpt\")\n",
    "vocoder.eval().cuda()\n",
    "\n",
    "\n",
    "vocoder22050 = HifiGanModel.restore_from(\"/home/pneekhara/PreTrainedModels/hifigan_ljspeech.nemo\")\n",
    "vocoder22050.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bde39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, taco_model_ckpt = _get_best_ckpt_from_experiment(\"/home/pneekhara/TacoRough44100\")\n",
    "print(taco_model_ckpt)\n",
    "spec_model = Tacotron2Model.load_from_checkpoint(taco_model_ckpt)\n",
    "# spec_model = Tacotron2Model.restore_from(\"/home/pneekhara/PreTrainedModels/Tacotron2-8051-char.nemo\")\n",
    "spec_model.cuda()\n",
    "spec, audio = infer_taco(spec_model, vocoder, \"This is a tricky text to speech example for comparing Taco tron and Fast pitch models.\")\n",
    "ipd.display(ipd.Audio(audio, rate=44100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e1a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, fastpitch_ckpt = _get_best_ckpt_from_experiment(\"/home/pneekhara/ExperimentsAutomatedResetPitch/8051_to_6097_no_mixing_1_mins\")\n",
    "print(fastpitch_ckpt)\n",
    "spec_model = FastPitchModel.load_from_checkpoint(fastpitch_ckpt)\n",
    "# spec_model = Tacotron2Model.restore_from(\"/home/pneekhara/PreTrainedModels/tts_en_tacotron2.nemo\")\n",
    "spec_model.cuda()\n",
    "spec, audio = infer(spec_model, vocoder, \"This is a tricky text to speech example for comparing Taco tron and Fast pitch models.\", speaker = 1)\n",
    "ipd.display(ipd.Audio(audio, rate=44100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f1da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "taco_model_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10abec0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spec_model_original = FastPitchModel.restore_from('/home/pneekhara/PreTrainedModels/FastPitch.nemo')\n",
    "spec_model_original.eval().cuda()\n",
    "# spec_model = FastPitchModel.load_from_checkpoint(\"/home/pneekhara/Experiments/8051to92Simple100/FastPitch/2021-07-19_09-30-44/checkpoints/FastPitch--v_loss=1.11-epoch=182.ckpt\")\n",
    "\n",
    "data_dir = \"/home/pneekhara/Datasets/78419/Hi_Fi_TTS_v_0_backup\"\n",
    "experiment_base_dir = \"/home/pneekhara/ExperimentsAutomatedResetPitch/\"\n",
    "clean_other_mapping = {\n",
    "    92 : 'clean',\n",
    "    6097 : 'clean'\n",
    "}\n",
    "\n",
    "full_data_ckpts = {\n",
    "    92 : '/home/pneekhara/Checkpoints/FastPitchSpeaker92Epoch999.ckpt',\n",
    "    6097 : '/home/pneekhara/Checkpoints/FastPitch6097Epoch999.ckpt'\n",
    "}\n",
    "num_val = 1\n",
    "\n",
    "for speaker in [6097, 92]:\n",
    "    manifest_path = os.path.join(data_dir, \"{}_manifest_{}_{}.json\".format(speaker, clean_other_mapping[speaker], \"dev\"))\n",
    "    val_records = []\n",
    "    with open(manifest_path, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            val_records.append( json.loads(line) )\n",
    "            if len(val_records) >= num_val:\n",
    "                break\n",
    "    print (\"**** REAL VALIDATION *****\")\n",
    "    for vidx, val_record in enumerate(val_records):\n",
    "        print(\"Audio path:\", val_record['audio_filepath'] )\n",
    "        audio_path = os.path.join( data_dir, val_record['audio_filepath'] )\n",
    "        \n",
    "        real_wav = wav_featurizer.process(audio_path)\n",
    "        real_mel, _ = mel_processor.get_features(real_wav[None], torch.tensor([[real_wav.shape[0]]]).long() )\n",
    "        real_mel = real_mel[0]\n",
    "#         imshow(real_mel[0].cpu().numpy(), origin=\"lower\", aspect = \"auto\")\n",
    "#         plt.show()\n",
    "        real_mel = real_mel.cuda()\n",
    "        with torch.no_grad():\n",
    "            # vocoded_audio_real = vocoder(x=real_mel.half()).squeeze(1)\n",
    "            vocoded_audio_real = vocoder.convert_spectrogram_to_audio(spec=real_mel).cpu().numpy()\n",
    "\n",
    "        # vocoded_audio_real = vocoded_audio_real.to('cpu').numpy()\n",
    "        # audio, sr = librosa.load(audio_path, sr=None)\n",
    "        print (vidx, val_record['text'])\n",
    "        print(\"Ground Truth Audio for speaker:\", speaker)\n",
    "        ipd.display(ipd.Audio(real_wav, rate=44100))\n",
    "        print(\"Ground truh spectrogram vocoded (HiFiGAN):\", speaker)\n",
    "        ipd.display(ipd.Audio(vocoded_audio_real, rate=44100))\n",
    "        # print(\"Vocoded (GL) from real spectrogram:\", speaker)\n",
    "    print (\"************************\")\n",
    "    print (\"********Generated*********\")\n",
    "    for duration_mins in [\"All\", 60, 5, 1]:\n",
    "        for mixing in [False, True]:\n",
    "            if duration_mins == \"All\":\n",
    "                if mixing:\n",
    "                    continue\n",
    "                last_ckpt = full_data_ckpts[speaker]\n",
    "            else:\n",
    "                model_ckpts, last_ckpt = get_best_ckpt(experiment_base_dir, speaker, duration_mins, mixing, 8051)\n",
    "            if last_ckpt is None:\n",
    "                print (\"Checkpoint not found for:\", \"Speaker: {} | Dataset size: {} mins | Mixing:{}\".format(speaker, duration_mins, mixing)) \n",
    "                continue\n",
    "                \n",
    "            # print(last_ckpt)\n",
    "            spec_model = FastPitchModel.load_from_checkpoint(last_ckpt)\n",
    "            spec_model.eval().cuda()\n",
    "            _speaker=None\n",
    "            if mixing:\n",
    "                _speaker = 1\n",
    "            for val_record in val_records:\n",
    "                print (\"SYNTHESIZED FOR -- Speaker: {} | Dataset size: {} mins | Mixing:{} | Text: {}\".format(speaker, duration_mins, mixing, val_record['text']))\n",
    "                spec, audio = infer(spec_model, vocoder, val_record['text'], speaker = _speaker)\n",
    "                ipd.display(ipd.Audio(audio, rate=44100))\n",
    "                %matplotlib inline\n",
    "                #if spec is not None:\n",
    "#                 imshow(spec, origin=\"lower\", aspect = \"auto\")\n",
    "#                 plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6111a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef68d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_generate = \"by nineteen fifty two this figure had become one hundred fifty four thousand two hundred seventy seven by virtue of minor changes', 'duration': 9.34, 'text_no_preprocessing': 'By 1952, this figure had become 154,277 by virtue of minor changes.\"\n",
    "spec, audio = infer(spec_model_original, vocoder, text_to_generate)\n",
    "print(\"Original model trained on speaker 8051\")\n",
    "ipd.display(ipd.Audio(audio, rate=44100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf30473",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocoder_ckpt_path = _get_best_ckpt_from_experiment(\"/home/pneekhara/ExperimentsHiFiJon/JonFinetuningMixing\")[1]\n",
    "print (vocoder_ckpt_path)\n",
    "vocoder_jon = HifiGanModel.load_from_checkpoint(vocoder_ckpt_path)\n",
    "vocoder_jon.eval().cuda()\n",
    "_, last_ckpt = _get_best_ckpt_from_experiment(\"/home/pneekhara/ExperimentsMainBranch/JonFinetuningMixed\")\n",
    "print(last_ckpt)\n",
    "spec_model_custom = FastPitchModel.load_from_checkpoint(last_ckpt).cuda()\n",
    "\n",
    "# text_to_generate = \"This is an experiment to see how good the model performs on real world data.\"\n",
    "# spec, audio = infer(spec_model_custom, vocoder_jon, text_to_generate)\n",
    "# print(\"Jon's synthetic voice - finetuned vocoder\")\n",
    "# ipd.display(ipd.Audio(audio, rate=44100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bfee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_manifest_file = \"/home/pneekhara/JonData/val_list.json\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(val_manifest_file) as f:\n",
    "    all_lines = f.read().split(\"\\n\")\n",
    "    for line in all_lines:\n",
    "        if len(line) > 0:\n",
    "            record = json.loads(line)\n",
    "            \n",
    "            print(\"Jon's actual voice\")\n",
    "            ipd.display(ipd.Audio(record['audio_filepath']))\n",
    "            \n",
    "            real_wav = wav_featurizer.process(record['audio_filepath'])\n",
    "            real_mel, _ = mel_processor.get_features(real_wav[None], torch.tensor([[real_wav.shape[0]]]).long() )\n",
    "            real_mel = real_mel[0]\n",
    "            real_mel = real_mel.cuda()\n",
    "            with torch.no_grad():\n",
    "#                 vocoded_audio_real = vocoder(x=real_mel.half()).squeeze(1)\n",
    "#                 vocoded_audio_real = vocoded_audio_real.to('cpu').numpy()\n",
    "                vocoded_audio_real = vocoder_jon.convert_spectrogram_to_audio(spec=real_mel).cpu().numpy()\n",
    "                print(\"Ground truh spectrogram vocoded (HiFiGAN):\")\n",
    "                ipd.display(ipd.Audio(vocoded_audio_real, rate=44100))\n",
    "            text_to_generate = record['text']\n",
    "            spec, audio = infer(spec_model_custom, vocoder_jon, text_to_generate, speaker=1)\n",
    "            print(\"Jon's synthetic voice - finetuned vocoder\")\n",
    "            ipd.display(ipd.Audio(audio, rate=44100))\n",
    "            \n",
    "            spec, audio = infer(spec_model_custom, vocoder, text_to_generate, speaker = 1)\n",
    "            print(\"Jon's synthetic voice - universal vocoder\")\n",
    "            ipd.display(ipd.Audio(audio, rate=44100))\n",
    "            print (\"*********************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa49544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_generate = \"This is Jon's voice. Does it sound like him?\"\n",
    "spec, audio = infer(spec_model_custom, vocoder_jon, text_to_generate, speaker = 1)\n",
    "print(\"Jon's synthetic voice - finetuned vocoder\")\n",
    "ipd.display(ipd.Audio(audio, \n",
    "                      rate=44100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f3ab29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemoenv",
   "language": "python",
   "name": "nemoenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
