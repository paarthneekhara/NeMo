{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nemo.collections.asr.models import EncDecSpeakerLabelModel\n",
    "from nemo.collections.asr.modules import AudioToMelSpectrogramPreprocessor\n",
    "from nemo.collections.asr.parts.preprocessing.features import WaveformFeaturizer\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import pickle\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython.display as ipd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e076bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing real and synthesized samples from various methods for different speakers.\n",
    "# samples_dir = \"/home/pneekhara/synthesized_samples_FINAL/\"\n",
    "samples_dir = \"/home/pneekhara/synthesized_samples_universalvoc/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62177827",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_paths = {}\n",
    "for fname in os.listdir(samples_dir):\n",
    "    if fname.endswith(\".wav\"):\n",
    "        wav_type, method_info, speaker, val_no = fname.split(\"_\")\n",
    "        val_no = val_no.split(\".\")[0]\n",
    "        key = \"{}_{}_{}\".format(wav_type, method_info, speaker)\n",
    "        if key in wav_paths:\n",
    "            wav_paths[key].append( ( int(val_no), os.path.join(samples_dir, fname) ) )\n",
    "        else:\n",
    "            wav_paths[key] = [ (int(val_no), os.path.join(samples_dir, fname)) ]\n",
    "\n",
    "for key in wav_paths:\n",
    "    wav_paths[key].sort()\n",
    "    wav_paths[key] = [ t[1] for t in wav_paths[key] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802677f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_paths.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33691b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speaker = \"92\"\n",
    "\n",
    "# for vidx in range(0,5):\n",
    "#     for wav_type in [\"real\", \"baseline\", \"synthesizedForceAlignment\", \"synthesized\"]:\n",
    "#         if wav_type == \"real\":\n",
    "#             method_info = \"actual\"\n",
    "#         elif wav_type == \"baseline\":\n",
    "#             method_info = \"originalModel\"\n",
    "#         else:\n",
    "#             method_info = \"1-nomix\"\n",
    "\n",
    "#         key = \"{}_{}_{}\".format(wav_type, method_info, speaker)\n",
    "#         wav_path = wav_paths[key][vidx]\n",
    "#         print (vidx, key)\n",
    "#         ipd.display(ipd.Audio(wav_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e364212",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_featurizer = WaveformFeaturizer(sample_rate=44100, int_values=False, augmentor=None)\n",
    "mel_processor = AudioToMelSpectrogramPreprocessor(\n",
    "        window_size = None,\n",
    "        window_stride = None,\n",
    "        sample_rate=44100,\n",
    "        n_window_size=2048,\n",
    "        n_window_stride=512,\n",
    "        window=\"hann\",\n",
    "        normalize=None,\n",
    "        n_fft=None,\n",
    "        preemph=None,\n",
    "        features=80,\n",
    "        lowfreq=0,\n",
    "        highfreq=None,\n",
    "        log=True,\n",
    "        log_zero_guard_type=\"add\",\n",
    "        log_zero_guard_value=1e-05,\n",
    "        dither=0.0,\n",
    "        pad_to=1,\n",
    "        frame_splicing=1,\n",
    "        exact_pad=False,\n",
    "        stft_exact_pad=False,\n",
    "        stft_conv=False,\n",
    "        pad_value=0,\n",
    "        mag_power=1.0\n",
    ")\n",
    "\n",
    "speaker_verification_model = EncDecSpeakerLabelModel.from_pretrained(\"speakerverification_speakernet\")\n",
    "# speaker_verification_model = EncDecSpeakerLabelModel.restore_from(\"/home/pneekhara/PreTrainedModels/ecapa_tdnn_mask.nemo\")\n",
    "speaker_verification_model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f62047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = \"/home/pneekhara/synthesized_audio_meta_dataFinal.pkl\"\n",
    "# pickle_path = \"/home/pneekhara/synthesized_audio_meta_data_universalvoc.pkl\"\n",
    "regenerate = False\n",
    "if os.path.exists(pickle_path) and not regenerate:\n",
    "    with open(pickle_path, \"rb\") as f:\n",
    "        meta_data = pickle.load(f)\n",
    "        embeddings = meta_data['embeddings']\n",
    "        spectrograms = meta_data['spectrograms']\n",
    "        pitch_contours = meta_data['pitch_contours']\n",
    "else:\n",
    "    embeddings = {}\n",
    "    spectrograms = {}\n",
    "    pitch_contours = {}\n",
    "    kidx = 0\n",
    "    for key in wav_paths:\n",
    "        print (\"Getting embeddings for:\", kidx, key, len(wav_paths[key]), len(wav_paths))\n",
    "        embeddings[key] = []\n",
    "        spectrograms[key] = []\n",
    "        pitch_contours[key] = []\n",
    "        for path in wav_paths[key]:\n",
    "            embedding = speaker_verification_model.get_embedding(path)\n",
    "            embeddings[key].append(embedding.cpu().numpy().flatten())\n",
    "\n",
    "            wav = wav_featurizer.process(path)\n",
    "            mel, _ = mel_processor.get_features(wav[None], torch.tensor([[wav.shape[0]]]).long() )\n",
    "            mel = mel[0][0].cpu().numpy()\n",
    "\n",
    "            pitch, _, _ = librosa.pyin(\n",
    "                wav.numpy(),\n",
    "                fmin=80,\n",
    "                fmax=640,\n",
    "                frame_length=2048,\n",
    "                sr=44100,\n",
    "                fill_na=0.0,\n",
    "            )\n",
    "\n",
    "            spectrograms[key].append(mel)\n",
    "            pitch_contours[key].append(pitch)\n",
    "        \n",
    "        kidx += 1\n",
    "\n",
    "    with open(pickle_path, \"wb\") as f:\n",
    "        meta_data = {\n",
    "            'spectrograms' : spectrograms,\n",
    "            'pitch_contours' : pitch_contours,\n",
    "            'embeddings' : embeddings\n",
    "        }\n",
    "        pickle.dump(meta_data, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc41e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import random\n",
    "def caclulate_speaker_verification_metrics(embeddings, duration_mins, mixing, eval_mode=\"synthetic\"):\n",
    "\n",
    "    def get_sim_score(v1, v2):\n",
    "        sim = np.dot(v1, v2)\n",
    "        sim /= (np.sqrt(np.dot(v1, v1)) * np.sqrt(np.dot(v2, v2)))\n",
    "        return sim\n",
    "        \n",
    "        \n",
    "    real_embeddings = {}\n",
    "    synthetic_embeddings = {}\n",
    "    \n",
    "    mix_str = \"mix\" if mixing else \"nomix\"\n",
    "    for key in embeddings:\n",
    "        speaker = key.split(\"_\")[-1]\n",
    "        if \"real_actual\" in key:\n",
    "            real_embeddings[speaker] = embeddings[key]\n",
    "        if \"synthesized_{}-{}\".format(duration_mins, mix_str) in key:\n",
    "            synthetic_embeddings[speaker] = embeddings[key]\n",
    "    \n",
    "    trial_pairs = []\n",
    "    if eval_mode == \"real\":\n",
    "        synthetic_embeddings = real_embeddings\n",
    "        \n",
    "    for speaker in sorted(synthetic_embeddings.keys()):\n",
    "        for emb_idx, emb in enumerate(synthetic_embeddings[speaker]):\n",
    "            pos_pairs = []\n",
    "            for pos_emb_idx in range(0, len(real_embeddings[speaker])):\n",
    "                if emb_idx != pos_emb_idx:\n",
    "                    sim_score = get_sim_score(emb, real_embeddings[speaker][pos_emb_idx])\n",
    "                    pos_pairs.append( (sim_score, 1))\n",
    "            neg_pairs = []\n",
    "            for neg_speaker in sorted(real_embeddings.keys()):\n",
    "                if neg_speaker != speaker:\n",
    "                    for neg_emb in real_embeddings[neg_speaker]:\n",
    "                        sim_score = get_sim_score(emb, neg_emb)\n",
    "                        neg_pairs.append( (sim_score, 0))\n",
    "            random.seed(42)\n",
    "            random.shuffle(neg_pairs)\n",
    "            # neg_pairs = neg_pairs[:len(pos_pairs)]\n",
    "            \n",
    "            trial_pairs += pos_pairs\n",
    "            trial_pairs += neg_pairs\n",
    "    \n",
    "    print (\"Num Trials\", len(trial_pairs))      \n",
    "    y_true = [pair[1] for pair in trial_pairs]\n",
    "    y_score = [pair[0] for pair in trial_pairs]\n",
    "    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_true, y_score, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    EER = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    EER_check = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    \n",
    "    assert abs(EER - EER_check) < 0.1\n",
    "    \n",
    "    return EER\n",
    "    \n",
    "    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd32b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mixing in [False, True]:\n",
    "    for duration_mins in [1, 5, 30, 60, 'All']:\n",
    "        if duration_mins == \"All\" and mixing:\n",
    "            continue\n",
    "        eer = caclulate_speaker_verification_metrics(embeddings, duration_mins, mixing, eval_mode=\"synthetic\")\n",
    "        print (eer)\n",
    "#         print ( \"finetuning mins:{} mixing:{} eer:{}\".format(duration_mins, mixing, eer) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73e800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "caclulate_speaker_verification_metrics(embeddings, 60, False, eval_mode=\"real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ec342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pitch_contour_error_metrics(ref_pitch_contour_np, gen_pitch_contour_np):\n",
    "    NFOE_error = 0.0 \n",
    "    NUV_ref = 0.0\n",
    "    NVU_gen = 0.0\n",
    "    NVV = 0.0\n",
    "    N_total = 0.0\n",
    "    non_zero_count = 0.0\n",
    "    for i in range(min(len(ref_pitch_contour_np), len(gen_pitch_contour_np))):\n",
    "        N_total += 1.\n",
    "        if (ref_pitch_contour_np[i] != 0 and gen_pitch_contour_np[i] != 0):        \n",
    "            non_zero_count +=1\n",
    "        if (ref_pitch_contour_np[i] == 0 and gen_pitch_contour_np[i] != 0):\n",
    "            NUV_ref += 1\n",
    "        if (gen_pitch_contour_np[i] == 0 and ref_pitch_contour_np[i] != 0):\n",
    "            NVU_gen += 1\n",
    "\n",
    "        if (ref_pitch_contour_np[i] > 0 and  gen_pitch_contour_np[i] > 0):\n",
    "            NVV += 1 \n",
    "            ratio = (gen_pitch_contour_np[i]/ref_pitch_contour_np[i])\n",
    "            if (ratio > 1.2 or ratio < 0.8):\n",
    "                NFOE_error +=1\n",
    "\n",
    "    FFE = ((NFOE_error + NUV_ref + NVU_gen)/N_total)\n",
    "\n",
    "    if NVV == 0:\n",
    "        GPE = 1.0\n",
    "    else:\n",
    "        GPE =  (NFOE_error/NVV)\n",
    "\n",
    "\n",
    "    VDE = ((NUV_ref + NVU_gen)/N_total)\n",
    "\n",
    "\n",
    "    metric_dict = {'FFE': FFE, \"GPE\" : GPE, \"VDE\" : VDE}\n",
    "\n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb1f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import pylab as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd7dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for speaker in [92, 6097]:\n",
    "    for mixing in [False, True]:\n",
    "        for finetuning_duration in [1, 5, 30, 60, 'All']:\n",
    "        \n",
    "            if mixing and finetuning_duration == 'All':\n",
    "                continue\n",
    "            mix_str = \"nomix\"\n",
    "            if mixing:\n",
    "                mix_str = \"mix\"\n",
    "            synthesized_key = \"synthesizedForceAlignment_{}-{}_{}\".format(finetuning_duration, mix_str, speaker)\n",
    "            actual_key = \"real_actual_{}\".format(speaker)\n",
    "            \n",
    "            mean_metrics = {\n",
    "                'FFE' : [],\n",
    "                'GPE' : [],\n",
    "                'VDE' : []\n",
    "            }\n",
    "            for idx in range(len(pitch_contours[actual_key])):\n",
    "                ref_pitch = pitch_contours[actual_key][idx]\n",
    "                synth_pitch = pitch_contours[synthesized_key][idx]\n",
    "                \n",
    "                pitch_metrics = calculate_pitch_contour_error_metrics(ref_pitch, synth_pitch)\n",
    "                for error_key in pitch_metrics:\n",
    "                    mean_metrics[error_key].append(pitch_metrics[error_key])\n",
    "            for error_key in mean_metrics:\n",
    "                mean_metrics[error_key] = round(float(np.mean(mean_metrics[error_key])),3)\n",
    "            \n",
    "            print (speaker, finetuning_duration, \"mixing:\", mixing, mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16bc7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/pneekhara/synthesized_samplesv3/speaking_rate.json\") as f:\n",
    "    speaking_rate_stats = json.loads(f.read())\n",
    "    for key in speaking_rate_stats:\n",
    "        for metric_key in speaking_rate_stats[key]:\n",
    "            print (key,metric_key, round(speaking_rate_stats[key][metric_key], 2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79191b25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for speaker in [92, 6097]:\n",
    "    for finetuning_duration in [5, 30, 60, 'All']:\n",
    "        for mixing in [True]:\n",
    "            if mixing and finetuning_duration == 'All':\n",
    "                continue\n",
    "            mix_str = \"nomix\"\n",
    "            if mixing:\n",
    "                mix_str = \"mix\"\n",
    "            synthesized_key = \"synthesizedForceAlignment_{}-{}_{}\".format(finetuning_duration, mix_str, speaker)\n",
    "            actual_key = \"real_actual_{}\".format(speaker)\n",
    "            \n",
    "            mean_metrics = {\n",
    "                'FFE' : [],\n",
    "                'GPE' : [],\n",
    "                'VDE' : []\n",
    "            }\n",
    "            for idx in range(len(pitch_contours[actual_key])):\n",
    "                ref_pitch = pitch_contours[actual_key][idx]\n",
    "                synth_pitch = pitch_contours[synthesized_key][idx]\n",
    "                \n",
    "                pitch_metrics = calculate_pitch_contour_error_metrics(ref_pitch, synth_pitch)\n",
    "                for error_key in pitch_metrics:\n",
    "                    mean_metrics[error_key].append(pitch_metrics[error_key])\n",
    "            for error_key in mean_metrics:\n",
    "                mean_metrics[error_key] = round(float(np.mean(mean_metrics[error_key])),3)\n",
    "            \n",
    "            print (speaker, finetuning_duration,  mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e3efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mscatter(x,y, ax=None, m=None, **kw):\n",
    "    import matplotlib.markers as mmarkers\n",
    "    #ax = ax or plt.gca()\n",
    "    sc = plt.scatter(x,y,**kw)\n",
    "    if (m is not None) and (len(m)==len(x)):\n",
    "        paths = []\n",
    "        for marker in m:\n",
    "            if isinstance(marker, mmarkers.MarkerStyle):\n",
    "                marker_obj = marker\n",
    "            else:\n",
    "                marker_obj = mmarkers.MarkerStyle(marker)\n",
    "            path = marker_obj.get_path().transformed(\n",
    "                        marker_obj.get_transform())\n",
    "            paths.append(path)\n",
    "        sc.set_paths(paths)\n",
    "    return sc\n",
    "\n",
    "def visualize_embeddings(embedding_dict_np, title = \"TSNE\"):\n",
    "    # extract num_speakers 10. With new speaker key will have num_speakers+1 speakers in the TSNE plot\n",
    "    # keys = []\n",
    "    label_list =[]\n",
    "    color =[]\n",
    "    marker_shape = []\n",
    "    color_idx = 0\n",
    "    universal_embed_list=[]\n",
    "    handle_list=[]  \n",
    "    _unique_speakers = {}\n",
    "    \n",
    "    marker_list = ['<', '*', 'h', 'X', 's', 'H', 'D', 'd', 'P', 'v', '^', '>', '8', 'p']\n",
    "    kidx = 0\n",
    "    \n",
    "    speaker_keys = {}\n",
    "    for key in embedding_dict_np:\n",
    "        wav_type, method_info, speaker = key.split(\"_\")\n",
    "        if speaker in speaker_keys:\n",
    "            speaker_keys[speaker].append(key)\n",
    "        else:\n",
    "            speaker_keys[speaker] = [key]\n",
    "    \n",
    "    _all_keys = []\n",
    "    for spk in speaker_keys:\n",
    "        _all_keys += speaker_keys[spk]\n",
    "        \n",
    "    for key in _all_keys:\n",
    "        wav_type, method_info, speaker = key.split(\"_\")\n",
    "        if method_info == \"vocoded\":\n",
    "            continue\n",
    "        if speaker not in _unique_speakers:\n",
    "            _unique_speakers[speaker] = len(_unique_speakers)\n",
    "        \n",
    "        _num_samples = len(embedding_dict_np[key])\n",
    "        if speaker in [\"92\", \"6097\"]:\n",
    "            _num_samples = 10 # keeping just 10 to keep things balanced with other speakers\n",
    "#         print (key, _num_samples)\n",
    "        universal_embed_list += embedding_dict_np[key][:_num_samples]\n",
    "        \n",
    "        id_color = plt.cm.tab20(_unique_speakers[speaker])\n",
    "        #id_color = plt.cm.tab20(kidx)\n",
    "        color_element = [id_color] * _num_samples\n",
    "        color += color_element\n",
    "        _marker_shape = [ marker_list[kidx % len(marker_list)] ] * _num_samples\n",
    "        marker_shape += _marker_shape\n",
    "        _label = \"{} : {}\".format(key, marker_list[kidx % len(marker_list)])\n",
    "        handle_list.append(mpatches.Patch(color = id_color, label=_label))\n",
    "        \n",
    "        kidx += 1\n",
    "        \n",
    "   \n",
    "    \n",
    "    speaker_embeddings = TSNE(n_components=2, random_state=0).fit_transform(universal_embed_list)        \n",
    "    \n",
    "    #ax = plt.axes()\n",
    "    #ax.set_facecolor(\"gainsboro\")\n",
    "    #plt.scatter(speaker_embeddings[:, 0], speaker_embeddings[:, 1], marker = marker_shape,  c=color, s=30)\n",
    "    mscatter(speaker_embeddings[:, 0], speaker_embeddings[:, 1], m = marker_shape,  c=color, s=60)\n",
    "   \n",
    "    plt.legend(handles=handle_list,title=\"Speaker_ID\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_pitch(pitch_actual, pitch_synthetic):\n",
    "    plt.plot(range(len(pitch_actual)), pitch_actual, \"r\", label = \"Real\" )\n",
    "    plt.plot(range(len(pitch_synthetic)), pitch_synthetic, \"b\", label = \"Synthetic\" )\n",
    "    plt.legend()\n",
    "    plt.title(\"F0 Pitch contour\")\n",
    "    plt.show()\n",
    "\n",
    "def visualize_synthesized_audio(speaker, duration, mixing, n=2):\n",
    "    actual_key = \"real_actual_{}\".format(speaker)\n",
    "    mix_str = \"mix\" if mixing else \"nomix\"\n",
    "    synthesized_key = \"synthesized_{}-{}_{}\".format(duration, mix_str, speaker)\n",
    "    \n",
    "    if duration == \"All\" and mixing:\n",
    "        return\n",
    "    \n",
    "    if duration == \"All\":\n",
    "        synth_msg = \"trained from scratch on all data audio of speaker {}\".format(speaker)\n",
    "    else:\n",
    "        synth_msg = \"finetuned on {} mins audio of speaker {}\".format(duration, speaker)\n",
    "        if mixing:\n",
    "            synth_msg += \" mixed with 5000 samples of original speaker.\"\n",
    "    \n",
    "    for idx in range(n):\n",
    "        \n",
    "#         actual_spectrogram = spectrograms[actual_key][idx]\n",
    "#         synthetic_spectrogram = spectrograms[synthesized_key][idx]\n",
    "        \n",
    "#         actual_pitch = pitch_contours[actual_key][idx]\n",
    "#         synthetic_pitch = pitch_contours[synthesized_key][idx]\n",
    "        \n",
    "        print (\"---\"*30)\n",
    "        \n",
    "        \n",
    "        print(idx+1, \"Actual audio\")\n",
    "#         imshow(actual_spectrogram, origin=\"lower\", aspect = \"auto\")\n",
    "#         plt.show()\n",
    "        \n",
    "        ipd.display(ipd.Audio(wav_paths[actual_key][idx]))\n",
    "        \n",
    "        \n",
    "        print(idx+1, \"Synthetic audio by model\", synth_msg)\n",
    "#         imshow(synthetic_spectrogram, origin=\"lower\", aspect = \"auto\")\n",
    "#         plt.show()\n",
    "        ipd.display(ipd.Audio(wav_paths[synthesized_key][idx]))\n",
    "        \n",
    "#         plot_pitch(actual_pitch, synthetic_pitch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc3103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (9, 3)\n",
    "\n",
    "for speaker in [6097]:\n",
    "    for finetuning_mins in [60, 5, \"All\"]:\n",
    "        for mixing in [False, True]:\n",
    "            visualize_synthesized_audio(speaker, finetuning_mins, mixing, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6c8c21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12, 12)\n",
    "visualize_embeddings(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b71c21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(embeddings['synthesized_5-nomix_6097'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import Tacotron2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58e974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tacotron2Model.list_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b4bfe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemoenv",
   "language": "python",
   "name": "nemoenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
