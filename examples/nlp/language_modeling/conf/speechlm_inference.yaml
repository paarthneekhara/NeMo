name: p_tuning_squad_t5
checkpoint_path: null
nemo_path: null # Supply either checkpoint_path or nemo_path, not both

mask_prob: 0.5
trainer:
  devices: 1
  accelerator: gpu
  num_nodes: 1
  precision: 16
  logger: False
  enable_checkpointing: False
  replace_sampler_ddp: False
  max_epochs: 1000
  max_steps: -1
  log_every_n_steps: 10
  val_check_interval: 1000
  gradient_clip_val: 1.0
  limit_val_batches: 25
  resume_from_checkpoint: null
  # resume_from_checkpoint: /datap/misc/Experiments3/SpeechExperiments/Aug2BugsFixed/2023-08-03_04-58-13/checkpoints/Epoch6000.ckpt

exp_manager:
  explicit_log_dir: null
  exp_dir: null
  name: ${name}
  create_wandb_logger: False
  wandb_logger_kwargs:
    project: PromptLearning-T5
    name: ${name}
  resume_if_exists: False
  resume_ignore_no_checkpoint: True
  create_checkpoint_callback: True
  checkpoint_callback_params:
    monitor: val_loss
    save_top_k: 2
    mode: min
    save_nemo_on_train_end: False # Should be false, correct prompt learning model file is saved at model.nemo_path set below
    filename: "megatron_t5_prompt_tune--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}"
    model_parallel_size: ${model.tensor_model_parallel_size}
    save_best_model: True
  create_early_stopping_callback: False

model:
  seed: 1234
  nemo_path: ${name}.nemo # .nemo filename/absolute path to where the virtual prompt model parameters will be saved
  virtual_prompt_style: "p-tuning" # one of 'prompt-tuning', 'p-tuning', or 'inference'
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  global_batch_size: 4
  micro_batch_size: 4 # micro batch size should equal global batch size when pipeline parallel = 1
  validation_global_batch_size: ${model.global_batch_size}
  validation_micro_batch_size: ${model.micro_batch_size}
  validation_drop_last: False
  report_validation_metric: False
  validation_metric: accuracy
  num_speech_tokens: 10112 # Vocabulary size pertaining to speech
  model_seq_length: 1536
  seq_pattern: "parallel" # parallel, delay_parallel, flatten
  speech_head_type: "token_level" # token_level, linear

  restore_path: null
  language_model_path: /datap/misc/Checkpoints/megatron_t5_220m/tp1_pp1/megatron_t5_expanded_vocab_posemb1536_220m.nemo
  save_nemo_on_validation_end: True # Saves an inference ready .nemo file every time a checkpoint is saved during training.
  existing_tasks: []
  new_tasks: ["squad"]


  task_templates:
  - taskname: "squad"
    prompt_template: "<|VIRTUAL_PROMPT_0|> {context} {question} {answer}"
    total_virtual_tokens: 5
    virtual_token_splits: [5]
    truncate_field: context
    answer_field: answer

  p_tuning: # P-tuning specific params
      encoder_type: "mlp" # Either "mlp" or "lstm", mlp is default
      num_layers: 2 # 2 recommended for MLP, 1 recommended for LSTM, must be at least 2 for mlp
      dropout: 0.0

  prompt_tuning: # Prompt tunin specific params
    new_prompt_init_methods: ['text'] # List of 'text' or 'random', should correspond to tasks listed in new tasks
    new_prompt_init_text: ['some init text goes here'] # some init text if init method is text, or None if init method is random

  data:
   # Path to data must be specified by the user.
    # Supports List, String and Dictionary
    # List : can override from the CLI: "model.data.data_prefix=[.5,/raid/data/pile/my-gpt3_00_text_document,.5,/raid/data/pile/my-gpt3_01_text_document]",
    # Or see example below: 
    # data_prefix: 
    #   - .5
    #   - /raid/data/pile/my-gpt3_00_text_document
    #   - .5
    #   - /raid/data/pile/my-gpt3_01_text_document
    # Dictionary: can override from CLI "model.data.data_prefix"={"train":[1.0, /path/to/data], "validation":/path/to/data, "test":/path/to/test}
    # Or see example below:
    # "model.data.data_prefix: {train:[1.0,/path/to/data], validation:[/path/to/data], test:[/path/to/test]}"
    # data_prefix: ["/datap/misc/BinaryDataset/books3/Books3_shuf_text_document"]
    # data_prefix: [1, "/datap/misc/BinaryDataset/librilight/eng_librivox_22khz_encodec_pt_filepath_document", 1, "/datap/misc/BinaryDataset/t5binaries/my-t5_test_bert_tokenizer_text_document"]
    data_prefix: ["/datap/misc/BinaryDataset/librilight/eng_librivox_22khz_encodec_pt_filepath_document"]
    index_mapping_dir: null # path to save index mapping .npy files, by default will save in the same location as data_prefix
    data_impl: "lazy"
    splits_string: 900,50,50
    seq_length: 512
    skip_warmup: True
    num_workers: 2
    dataloader_type: single # cyclic
    reset_position_ids: False # Reset position ids after end-of-document token
    reset_attention_mask: False # Reset attention mask after end-of-document token
    eod_mask_loss: False # Mask loss for the end of document tokens
    validation_drop_last: True # Set to false if the last partial validation samples is to be consumed
    no_seqlen_plus_one_input_tokens: False # Set to True to disable fetching (sequence length + 1) input tokens, instead get (sequence length) input tokens and mask the last token
    pad_samples_to_global_batch_size: False # Set to True if you want to pad the last partial batch with -1's to equal global batch size
    shuffle_documents: True # Set to False to disable documents shuffling. Sample index will still be shuffled
    exchange_indices_distributed: False # Set to True to exchange indices via torch.distributed instead of filesystem

  optim:
    name: fused_adam
    lr: 1e-4
    weight_decay: 0.01
    betas:
    - 0.9
    - 0.98
    sched:
      name: CosineAnnealing
      warmup_steps: 100
      constant_steps: 0
      min_lr: 0.0
      monitor: val_loss
      reduce_on_plateau: false